{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#https://ieeexplore.ieee.org/abstract/document/9033031\n",
        "#https://www.researchgate.net/profile/Abdul-Muntakim-Rafi/publication/337366713_Image-based_Bengali_Sign_Language_Alphabet_Recognition_for_Deaf_and_Dumb_Community/links/5f047f5e92851c52d61df29e/Image-based-Bengali-Sign-Language-Alphabet-Recognition-for-Deaf-and-Dumb-Community.pdf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "import tensorflow as tf\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "fid = drive.ListFile({'q':\"title='sign.zip'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('sign.zip')\n",
        "f.keys()\n",
        "!unzip sign.zip"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-04-28T09:23:24.518436Z",
          "iopub.execute_input": "2023-04-28T09:23:24.518953Z",
          "iopub.status.idle": "2023-04-28T09:23:33.301168Z",
          "shell.execute_reply.started": "2023-04-28T09:23:24.518862Z",
          "shell.execute_reply": "2023-04-28T09:23:33.299605Z"
        },
        "trusted": true,
        "id": "lU02e_0eN-qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_logical_devices('GPU')\n",
        "stg=tf.distribute.MirroredStrategy(gpus)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T09:23:33.303532Z",
          "iopub.execute_input": "2023-04-28T09:23:33.304176Z",
          "iopub.status.idle": "2023-04-28T09:23:33.350508Z",
          "shell.execute_reply.started": "2023-04-28T09:23:33.304136Z",
          "shell.execute_reply": "2023-04-28T09:23:33.349498Z"
        },
        "trusted": true,
        "id": "Jf64bf14N-qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T09:23:33.354983Z",
          "iopub.execute_input": "2023-04-28T09:23:33.355374Z",
          "iopub.status.idle": "2023-04-28T09:23:33.361639Z",
          "shell.execute_reply.started": "2023-04-28T09:23:33.355339Z",
          "shell.execute_reply": "2023-04-28T09:23:33.360128Z"
        },
        "trusted": true,
        "id": "MRIMlN6UN-qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "folder_dir = '/content/RESIZED_DATASET'\n",
        "SIZE = 224\n",
        "DOWNSAMPLE_RATIO = 4\n",
        "JPEG_QUALITY = 100\n",
        "\n",
        "total_files = sum(len(files) for _, _, files in os.walk(folder_dir))\n",
        "\n",
        "with tqdm(total=total_files, desc=\"Processing Images\") as pbar:\n",
        "    for folder in os.listdir(folder_dir):\n",
        "        for file in os.listdir(os.path.join(folder_dir, folder)):\n",
        "                image_path = os.path.join(folder_dir, folder, file)\n",
        "                img = cv2.imread(image_path)\n",
        "                img_resized = cv2.resize(img, (SIZE,SIZE))\n",
        "                cv2.imwrite(image_path, img_resized)\n",
        "                pbar.update(1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T09:23:33.364823Z",
          "iopub.execute_input": "2023-04-28T09:23:33.365322Z",
          "iopub.status.idle": "2023-04-28T09:23:57.279118Z",
          "shell.execute_reply.started": "2023-04-28T09:23:33.365254Z",
          "shell.execute_reply": "2023-04-28T09:23:57.277967Z"
        },
        "trusted": true,
        "id": "An2tXbhyN-qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f6acde-7033-44cf-ebb1-7ab7d94aeaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Images: 100%|██████████| 11061/11061 [00:34<00:00, 316.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-04-28T09:23:57.280504Z",
          "iopub.execute_input": "2023-04-28T09:23:57.280986Z",
          "iopub.status.idle": "2023-04-28T09:23:57.290975Z",
          "shell.execute_reply.started": "2023-04-28T09:23:57.280939Z",
          "shell.execute_reply": "2023-04-28T09:23:57.289850Z"
        },
        "trusted": true,
        "id": "vB9FpCDeN-qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "picture_size = (224, 224)\n",
        "train_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=folder_dir,\n",
        "    shuffle=True,\n",
        "    image_size=picture_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed = 22\n",
        ")\n",
        "\n",
        "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=folder_dir,\n",
        "    shuffle=True,\n",
        "    image_size=picture_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed = 22\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T09:23:57.309237Z",
          "iopub.execute_input": "2023-04-28T09:23:57.309624Z",
          "iopub.status.idle": "2023-04-28T09:23:57.792544Z",
          "shell.execute_reply.started": "2023-04-28T09:23:57.309593Z",
          "shell.execute_reply": "2023-04-28T09:23:57.791270Z"
        },
        "trusted": true,
        "id": "WxNb7GsCN-qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce9ccd2-cf64-4ff5-81e7-0709438b1af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11061 files belonging to 38 classes.\n",
            "Using 8849 files for training.\n",
            "Found 11061 files belonging to 38 classes.\n",
            "Using 2212 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation, RandomTranslation, RandomFlip, RandomZoom\n",
        "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.keras.optimizers import RMSprop,SGD,Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "no_of_classes = 38\n",
        "\n",
        "with stg.scope():\n",
        "    img_augmentation = Sequential(\n",
        "    [\n",
        "        RandomRotation(factor=0.2),\n",
        "        RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "        RandomZoom(height_factor=0.2)\n",
        "    ],\n",
        "    name=\"img_augmentation\"\n",
        "    )\n",
        "\n",
        "    base_model = Xception(weights=\"imagenet\", input_shape=(SIZE, SIZE, 3), include_top=False)\n",
        "    inputs = Input(shape=(SIZE, SIZE, 3))\n",
        "    x = img_augmentation(inputs)\n",
        "    outputs = base_model(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(outputs)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    predictions = Dense(no_of_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(lr=0.001),\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T09:24:40.193314Z",
          "iopub.execute_input": "2023-04-28T09:24:40.193761Z",
          "iopub.status.idle": "2023-04-28T09:24:48.262221Z",
          "shell.execute_reply.started": "2023-04-28T09:24:40.193729Z",
          "shell.execute_reply": "2023-04-28T09:24:48.260947Z"
        },
        "trusted": true,
        "id": "wwbS3e5xN-qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4dec5e-e333-4575-bf4f-4e122820484b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " img_augmentation (Sequentia  (None, 224, 224, 3)      0         \n",
            " l)                                                              \n",
            "                                                                 \n",
            " xception (Functional)       (None, 7, 7, 2048)        20861480  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                16416     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 38)                1254      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,928,238\n",
            "Trainable params: 21,873,710\n",
            "Non-trainable params: 54,528\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                 factor=0.1,\n",
        "                                 patience=2,\n",
        "                                 verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                patience=5,\n",
        "                                verbose=1)\n",
        "\n",
        "callbacks = [lr_scheduler, early_stopping]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-28T09:24:50.807743Z",
          "iopub.execute_input": "2023-04-28T09:24:50.808573Z",
          "iopub.status.idle": "2023-04-28T09:24:50.815591Z",
          "shell.execute_reply.started": "2023-04-28T09:24:50.808514Z",
          "shell.execute_reply": "2023-04-28T09:24:50.814542Z"
        },
        "trusted": true,
        "id": "YEDYd-nIN-qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.trainable = False\n",
        "\n",
        "history = model.fit(train_set, epochs=100, validation_data=validation_set, callbacks=callbacks,\n",
        "                    steps_per_epoch=len(train_set), validation_steps=len(validation_set))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KIULoyrPN-qc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe4d3d9-1f3d-4dae-ffa9-a45565b9ef40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "277/277 [==============================] - 202s 559ms/step - loss: 3.5961 - accuracy: 0.0325 - val_loss: 21.5661 - val_accuracy: 0.0249 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "277/277 [==============================] - 150s 542ms/step - loss: 3.2511 - accuracy: 0.0640 - val_loss: 4.0756 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "277/277 [==============================] - 150s 540ms/step - loss: 2.7849 - accuracy: 0.1355 - val_loss: 3.1740 - val_accuracy: 0.1108 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "277/277 [==============================] - 151s 543ms/step - loss: 2.2477 - accuracy: 0.2364 - val_loss: 2.2606 - val_accuracy: 0.2663 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "277/277 [==============================] - 150s 543ms/step - loss: 1.8165 - accuracy: 0.3578 - val_loss: 1.6811 - val_accuracy: 0.3996 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "277/277 [==============================] - 151s 543ms/step - loss: 1.4980 - accuracy: 0.4450 - val_loss: 1.6136 - val_accuracy: 0.4652 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "277/277 [==============================] - 150s 542ms/step - loss: 1.2612 - accuracy: 0.5361 - val_loss: 1.2728 - val_accuracy: 0.5429 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "277/277 [==============================] - 150s 542ms/step - loss: 1.0717 - accuracy: 0.6101 - val_loss: 1.1851 - val_accuracy: 0.5918 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "277/277 [==============================] - 151s 544ms/step - loss: 0.8928 - accuracy: 0.6884 - val_loss: 0.9390 - val_accuracy: 0.6817 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "277/277 [==============================] - 150s 543ms/step - loss: 0.7571 - accuracy: 0.7413 - val_loss: 0.5451 - val_accuracy: 0.7988 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "277/277 [==============================] - 151s 544ms/step - loss: 0.6943 - accuracy: 0.7663 - val_loss: 0.6596 - val_accuracy: 0.7613 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "277/277 [==============================] - ETA: 0s - loss: 0.5861 - accuracy: 0.8048\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "277/277 [==============================] - 151s 545ms/step - loss: 0.5861 - accuracy: 0.8048 - val_loss: 0.5704 - val_accuracy: 0.8201 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "277/277 [==============================] - 151s 544ms/step - loss: 0.4218 - accuracy: 0.8625 - val_loss: 0.3073 - val_accuracy: 0.9082 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "277/277 [==============================] - 151s 545ms/step - loss: 0.3513 - accuracy: 0.8868 - val_loss: 0.3044 - val_accuracy: 0.9096 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "277/277 [==============================] - 152s 548ms/step - loss: 0.3271 - accuracy: 0.8952 - val_loss: 0.2992 - val_accuracy: 0.9064 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "277/277 [==============================] - 151s 545ms/step - loss: 0.2979 - accuracy: 0.9050 - val_loss: 0.2804 - val_accuracy: 0.9155 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "277/277 [==============================] - 152s 547ms/step - loss: 0.2920 - accuracy: 0.9084 - val_loss: 0.2559 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "277/277 [==============================] - 151s 546ms/step - loss: 0.2699 - accuracy: 0.9105 - val_loss: 0.2593 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "277/277 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.9177\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "277/277 [==============================] - 151s 546ms/step - loss: 0.2617 - accuracy: 0.9177 - val_loss: 0.2741 - val_accuracy: 0.9177 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "277/277 [==============================] - 151s 546ms/step - loss: 0.2465 - accuracy: 0.9202 - val_loss: 0.2623 - val_accuracy: 0.9231 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "277/277 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.9247\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "277/277 [==============================] - 151s 545ms/step - loss: 0.2376 - accuracy: 0.9247 - val_loss: 0.2574 - val_accuracy: 0.9245 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "277/277 [==============================] - 152s 549ms/step - loss: 0.2244 - accuracy: 0.9286 - val_loss: 0.2581 - val_accuracy: 0.9236 - lr: 1.0000e-06\n",
            "Epoch 22: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_dir = '/content/RESIZED_TESTING_DATA'\n",
        "test_datagen = ImageDataGenerator()\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=picture_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "5qss13uXp9Vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb4101d-1654-4c6d-8d01-83f39872b70d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1520 images belonging to 38 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e884a9e4762c>:10: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  test_loss, test_acc = model.evaluate_generator(test_generator)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9217105507850647\n"
          ]
        }
      ]
    }
  ]
}
